{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *A Corpus-Based Approach to Colour, Shape and Typography in Logos*\n",
    "\n",
    "This notebook contains the analyses performed for the chapter *A Corpus-Based Approach to Colour, Shape and Typography in Logos* by Christian MosbÃ¦k Johannessen, Mads Lomholt Tvede, Kristoffer Claussen Boesen and Tuomo Hiippala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "from scipy.stats import normaltest, mannwhitneyu, ttest_ind\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from PIL import Image\n",
    "import matplotlib.transforms as transforms\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Render matplotlib images in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which principal components you want to compare?\n",
    "\n",
    "Provide a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = [(\"PC1\", \"PC2\"), (\"PC1\", \"PC3\"), (\"PC1\", \"PC4\"), (\"PC1\", \"PC5\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Load the data and apply z-score normalisation to the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file; assign result to pandas DataFrame 'df'\n",
    "df = pd.read_csv(\"corpus_data.csv\", sep=\";\", decimal=',')\n",
    "\n",
    "# Take all rows [:] in the DataFrame but include only columns with data [3:]\n",
    "input_data = df.loc[:, df.columns[3:]]\n",
    "\n",
    "# Scale the data to zero mean and unit variance by applying the function 'scale' from sklearn.preprocessing\n",
    "df[input_data.columns] = input_data.apply(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data for outliers by getting Boolean (True/False) values for each data cell.\n",
    "\n",
    "Z-scores follow a normal distribution, so we know that 99.9% of data lie within four standard deviations on either side (-4 and +4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get boolean values for filtering the data\n",
    "over = ~df.loc[:, df.columns[3:]].apply(lambda x: x >= 4)\n",
    "under = ~df.loc[:, df.columns[3:]].apply(lambda x: x <= -4)\n",
    "\n",
    "# Filter the data for outliers\n",
    "df[input_data.columns] = df[input_data.columns][over]\n",
    "df[input_data.columns] = df[input_data.columns][under]\n",
    "\n",
    "# Drop outlier rows\n",
    "df = df.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with 45 logos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Organization'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis (PCA)\n",
    "\n",
    "Initialize and fit PCA model with **five** principal components to the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA model\n",
    "pca = PCA(n_components=5).fit(df.loc[:, df.columns[3:]])\n",
    "\n",
    "print(f\"Percentage of variance explained by principal components: {np.sum(pca.explained_variance_ratio_).round(3) * 100}%\")\n",
    "\n",
    "# Map variance explained by PC to dictionary\n",
    "vmap = {f\"PC{k}\": v for k, v in enumerate(pca.explained_variance_ratio_.round(2).tolist(), start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Plot the principal components against explained variance \n",
    "plt.bar(list(range(1, len(pca.explained_variance_ratio_) + 1)), (pca.explained_variance_ratio_ * 100), width=0.6)\n",
    "\n",
    "# Set labels\n",
    "plt.xlabel(\"Principal component\", fontsize=14)\n",
    "plt.ylabel(\"Percentage of variation explained\", fontsize=14)\n",
    "\n",
    "# Save figure on disk\n",
    "plt.savefig(f\"plot_{len(pca.explained_variance_ratio_)}_principal_components.pdf\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the PCA model to transform the input data and add the result to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform input data and get principal components\n",
    "X = pca.transform(df.loc[:, df.columns[3:]])\n",
    "\n",
    "# Get the shape of the second dimension in X for the number of principal components\n",
    "n_pc = X.shape[1]\n",
    "\n",
    "# Add principal components to the dataframe\n",
    "for x in range(0, n_pc):\n",
    "\n",
    "    # Assign the principal components to column PCx\n",
    "    df[f'PC{x+1}'] = X[:, x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define supporting functions for drawing confidence ellipses and adding logo thumbnails to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_ellipse(x, y, ax, n_std=2.0, facecolor='none', **kwargs):\n",
    "    \"\"\"\n",
    "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x, y : array-like, shape (n, )\n",
    "        Input data.\n",
    "\n",
    "    ax : matplotlib.axes.Axes\n",
    "        The axes object to draw the ellipse into.\n",
    "\n",
    "    n_std : float\n",
    "        The number of standard deviations to determine the ellipse's radiuses.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    matplotlib.patches.Ellipse\n",
    "\n",
    "    Other parameters\n",
    "    ----------------\n",
    "    kwargs : `~matplotlib.patches.Patch` properties\n",
    "    \"\"\"\n",
    "    if x.size != y.size:\n",
    "        raise ValueError(\"x and y must be the same size\")\n",
    "\n",
    "    cov = np.cov(x, y)\n",
    "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensionl dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0),\n",
    "                      width=ell_radius_x * 2,\n",
    "                      height=ell_radius_y * 2,\n",
    "                      fill=True, alpha=0.2,\n",
    "                      facecolor=facecolor, **kwargs)\n",
    "\n",
    "    # Calculating the standard deviation of x from\n",
    "    # the square root of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "    mean_x = np.mean(x)\n",
    "\n",
    "    # calculating the stdandard deviation of y ...\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "    mean_y = np.mean(y)\n",
    "\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mean_x, mean_y)\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    \n",
    "    return ax.add_patch(ellipse)\n",
    "\n",
    "# Define a function for processing logo images\n",
    "def plot_image(path):\n",
    "    \n",
    "    # Read image\n",
    "    img = Image.open(path).resize((30, 30), Image.ANTIALIAS)    \n",
    "    \n",
    "    return OffsetImage(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the principal components against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize seaborn nicer plots and a matplotlib figure\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Choose what to plot: 'thumbnails' includes thumbnails of logos, 'ellipse' draws confidence ellipses and 'kde' plots kernel density estimation\n",
    "thumbnails=False\n",
    "ellipse=True\n",
    "kde=False\n",
    "\n",
    "# Check what kind of plot layout is required; this depends on the number of components to plot\n",
    "if len(pcs) == 1:\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n",
    "    \n",
    "if len(pcs) == 2:\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n",
    "    \n",
    "if len(pcs) > 2:\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=int(round(len(pcs) / 2)), ncols=2, figsize=(12, 12))\n",
    "    fig.tight_layout(pad=6.0)\n",
    "    \n",
    "    axs = axs.flatten()\n",
    "\n",
    "# Define a colormap\n",
    "\n",
    "# Map the diagram type and colours into a dictionary\n",
    "color_dict = dict(zip(df['Organization'].unique(), sns.color_palette()))\n",
    "\n",
    "# Get unique organization types\n",
    "organizations = df['Organization'].unique().tolist()\n",
    "\n",
    "# Loop over the principal component tuples and axes\n",
    "for (pc_1, pc_2), ax in zip(pcs, axs):\n",
    "    \n",
    "    # Set axis shape\n",
    "    ax.set_aspect(1)\n",
    "    ax.set(xlim=(-4, 4), ylim=(-4, 4))\n",
    "\n",
    "    # Loop over organizations and plot their principal components\n",
    "    for org in organizations:\n",
    "\n",
    "        # Get data for specific organization type\n",
    "        slice = df.loc[df['Organization'] == org]\n",
    "        \n",
    "        # Plot confidence ellipse\n",
    "        if ellipse:\n",
    "            \n",
    "            # Plot confidence ellipse\n",
    "            confidence_ellipse(x=slice[pc_1], y=slice[pc_2], n_std=2, ax=ax, facecolor=color_dict[org])\n",
    "            \n",
    "        # Plot kernel density estimation\n",
    "        if kde:\n",
    "            \n",
    "            # Plot kernel density estimation\n",
    "            sns.kdeplot(slice[pc_1], slice[pc_2], ax=ax, shade=True, shade_lowest=False, n_levels=5, \n",
    "                        alpha=0.5, cbar=True, cbar_kws={\"shrink\": 0.875})\n",
    "\n",
    "        # Plot the data\n",
    "        ax.scatter(x=slice[pc_1], y=slice[pc_2], s=14, label=org)\n",
    "        \n",
    "        # Check if thumbnail images should be added to the plot\n",
    "        if thumbnails:\n",
    "\n",
    "            # Get X-Y coordinates and filenames\n",
    "            x, y, img = slice[pc_1], slice[pc_2], slice['File name']\n",
    "            \n",
    "            # Loop over coordinates and image filenames\n",
    "            for x_coord, y_coord, img_filename in zip(x, y, img):\n",
    "\n",
    "                # Create a bounding box for annotation\n",
    "                ab = AnnotationBbox(plot_image(f\"logos/{img_filename}\"), (x_coord, y_coord), frameon=False)\n",
    "\n",
    "                # Add the annotation box to the axis\n",
    "                ax.add_artist(ab)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    ax.set_xlabel(f\"{pc_1} (variance explained: {np.round(vmap[pc_1] * 100, 2)}%)\", fontsize=16)\n",
    "    ax.set_ylabel(f\"{pc_2} (variance explained: {np.round(vmap[pc_2] * 100, 2)}%)\", fontsize=16)\n",
    "    ax.set_title(f\"{pc_1} vs. {pc_2}\", fontsize=18)\n",
    "\n",
    "    # Add legend to the plot\n",
    "    ax.legend()\n",
    "        \n",
    "# Save figure on disk\n",
    "plt.savefig(f\"{pc_1}_vs_{pc_2}_{'kde' if kde else 'pca'}_plot.pdf\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate PCA loadings, that is, correlations between the original variables and the principal components derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a PCA loadings matrix for examining the correlation between original variables and PC\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "# Convert to a DataFrame\n",
    "loadings = pd.DataFrame(loadings.round(4), columns=df.filter(like='PC').columns.tolist(), index=df.columns[3:-n_pc])\n",
    "\n",
    "# Print the PCA loadings\n",
    "loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A statistical comparison of principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the principal components are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each organisation in the list 'organizations'\n",
    "for org in organizations:\n",
    "    \n",
    "    # Loop over each column for principal components; fetch column names from DataFrame 'loadings'\n",
    "    for pc in loadings.columns:\n",
    "        \n",
    "        # Test for normality: the data is retrieved from DataFrame 'df'\n",
    "        n_test = normaltest(df.loc[df['Organization'] == org][pc].to_numpy())\n",
    "        \n",
    "        # Print the result\n",
    "        if n_test[1] > 0.05:\n",
    "            \n",
    "            print(f\"{org}, {pc}, NORMAL.\")\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print(f\"{org}, {pc}, NOT NORMAL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Mann-Whitney U-test to compare the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the principal components\n",
    "for pc in loadings.columns:\n",
    "    \n",
    "    # Perform Mann-Whitney U-test on data fetched from the DataFrame 'df'\n",
    "    mw_test = mannwhitneyu(df.loc[df['Organization'] == 'OIL'][pc].to_numpy(), df.loc[df['Organization'] == 'NGO'][pc].to_numpy())\n",
    "        \n",
    "    # Print the result\n",
    "    if mw_test[1] > 0.05:\n",
    "            \n",
    "        print(f\"{pc} Not statistically significant at {np.round(mw_test[0], 3)}, {np.round(mw_test[1], 3)}.\")\n",
    "        \n",
    "    if mw_test[1] < 0.05:\n",
    "        \n",
    "        print(f\"{pc} Statistically significant at {np.round(mw_test[0], 3)}, {np.round(mw_test[1], 3)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
